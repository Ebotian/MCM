{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43.78409, 40.59322, 39.725663, 37.31208, 39.0, 36.600706, 37.048543, 34.923076, 38.327956, 41.123674, 35.568493, 38.151639, 34.312252, 33.364197, 34.539772, 41.554794, 41.755, 41.395833, 34.773584, 34.866141, 40.021276, 35.890173, 31.554054, 37.925, 40.95238, 33.7, 37.491124, 42.31088, 37.605633, 47.819767, 47.070707]\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from re import T\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import markovify\n",
    "id=0\n",
    "\n",
    "############ load and digitize the data\n",
    "data=pd.read_csv('/home/ebotian/MCM/tennis2.csv')\n",
    "\n",
    "############\n",
    "\n",
    "############# pre-process the data\n",
    "    # fill nan with 0, and replace AD with 50\n",
    "def pre_process(data):\n",
    "    data = pd.get_dummies(data, columns=['winner_shot_type','serve_width','serve_depth','return_depth'])\n",
    "    data = data.fillna(0)\n",
    "    data = data.replace('AD', 50.0)\n",
    "    #data['point_victor']=data[\"point_victor\"].replace(2,0)\n",
    "    #print(data.iloc[:,15])\n",
    "    ###################################\n",
    "    #for predict:\n",
    "    conditions = [\n",
    "        (data['server'] == 1) & (data['point_victor'] == 1),\n",
    "        (data['server'] == 1) & (data['point_victor'] == 2),\n",
    "        (data['server'] == 2) & (data['point_victor'] == 2),\n",
    "        (data['server'] == 2) & (data['point_victor'] == 1)\n",
    "    ]\n",
    "    choices = ['P1_serve_win', 'P1_serve_lose', 'P2_serve_win', 'P2_serve_lose']\n",
    "    data['state'] = np.select(conditions, choices, default='unknown')\n",
    "\n",
    "    # split the data into different match\n",
    "    grouped = dict(tuple(data.groupby(data['match_id'].ne(data['match_id'].shift()).cumsum())))\n",
    "\n",
    "    # Rename the subdata\n",
    "    subdata = {df['match_id'].iloc[0]: df for _, df in grouped.items()}\n",
    "\n",
    "    # Create a new dataset from the first column, excluding duplicates\n",
    "    match = pd.DataFrame(data.iloc[:, 0].drop_duplicates()).iloc[:,0].tolist()\n",
    "\n",
    "\n",
    "    return subdata,match\n",
    "\n",
    "subdata,match=pre_process(data)\n",
    "#print(subdata[match[1]])\n",
    "##############\n",
    "\n",
    "def process_all_ids(subdata):\n",
    "    features={}\n",
    "    target={}\n",
    "    for id in range(len(match)):\n",
    "        #index_array = subdata[match[id]][subdata[match[id]]['server'] == 2].index.values\n",
    "#        subdata[match[id]].loc[index_array, 'point_victor'] = 1 - subdata[match[id]].loc[index_array, 'point_victor']\n",
    "        target[match[id]]=pd.DataFrame(subdata[match[id]][\"point_victor\"])\n",
    "        # Add the \"elapsed_time\" column to the \"target\" DataFrame\n",
    "        subdata[match[id]]['elapsed_time'] = pd.to_timedelta(subdata[match[id]]['elapsed_time'])\n",
    "        #target[match[id]].insert(0, 'elapsed_time', subdata[match[id]]['elapsed_time'])\n",
    "        #target['elapsed_time'] = target['elapsed_time'].dt.total_seconds()\n",
    "        features[match[id]]=subdata[match[id]].drop(columns=[\"point_victor\"]).iloc[:,4:]\n",
    "        subdata[match[id]]=subdata[match[id]].drop(columns=[\"point_victor\"])\n",
    "\n",
    "    return target,features,subdata,index_array\n",
    "\n",
    "# Replace with your actual match ids\n",
    "target,features,subdata,index_array=process_all_ids(subdata)\n",
    "#print(subdata[match[1]])\n",
    "##############\n",
    "##############\n",
    "#invert the victor when the server is 2 to get server_victor\n",
    "#and after the prediction, we invert the victor again\n",
    "##############\n",
    "# Calculate the time difference between consecutive rows\n",
    "def get_average_interval(id, subdata):\n",
    "\n",
    "    # Calculate the time difference between consecutive rows\n",
    "    subdata[match[id]]['time_diff'] = subdata[match[id]]['elapsed_time'].diff()\n",
    "\n",
    "    # Calculate the 5th and 95th percentiles\n",
    "    lower_threshold = subdata[match[id]]['time_diff'].quantile(0.05)\n",
    "    upper_threshold = subdata[match[id]]['time_diff'].quantile(0.95)\n",
    "\n",
    "    # Exclude the top 5% and bottom 5% of periods\n",
    "    filtered_diff = subdata[match[id]]['time_diff'][(subdata[match[id]]['time_diff'] > lower_threshold) & (subdata[match[id]]['time_diff'] < upper_threshold)]\n",
    "\n",
    "    # Calculate the average of the remaining intervals\n",
    "    average_interval = filtered_diff.mean()\n",
    "    return average_interval.total_seconds()\n",
    "\n",
    "average_interval_time=[]\n",
    "\n",
    "for id in range(len(match)):\n",
    "    average_interval_time.append(get_average_interval(id, subdata))\n",
    "\n",
    "print(average_interval_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ add features\n",
    "add_feature=[]\n",
    "add_feature.append(\"p1_net_pt_probability\")\n",
    "add_feature.append(\"p2_net_pt_probability\")\n",
    "add_feature.append(\"p1_break_pt_probability\")\n",
    "add_feature.append(\"p2_break_pt_probability\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## defining the new features\n",
    "#for id in range(len(match)):\n",
    "#  for cnt in range(len(add_feature)):\n",
    "#    features[match[id]][add_feature[cnt]] = subdata[match[id]]['p1_net_pt']/ subdata[match[id]][\"p1_net_pt_won\"]\n",
    "#    features[match[id]][add_feature[cnt]] = subdata[match[id]]['p2_net_pt']/ subdata[match[id]][\"p2_net_pt_won\"]\n",
    "#    features[match[id]][add_feature[cnt]] = subdata[match[id]][\"p1_break_pt\"]/ subdata[match[id]][\"p1_break_pt_won\"]\n",
    "#    features[match[id]][add_feature[cnt]] = subdata[match[id]][\"p2_break_pt\"]/ subdata[match[id]][\"p2_break_pt_won\"]\n",
    "\n",
    "\n",
    "#data[add_feature_1] = data['p1_games'] - data['p2_games']\n",
    "#data[add_feature_2] = data['p1_winner'] / (data['p1_winner'] + data['p2_winner'])\n",
    "#data[add_feature_3] = data['p2_winner'] / (data['p1_winner'] + data['p2_winner'])\n",
    "#data[add_feature_4] = data['p1_break_pt_won'] / (data['p1_break_pt_won'] + data['p1_break_pt_missed'])\n",
    "#data[add_feature_5] = data['p2_break_pt_won'] / (data['p2_break_pt_won'] + data['p2_break_pt_missed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into features and target\n",
    "#target=pd.DataFrame(subdata[match[id]][\"point_victor\"])\n",
    "#\n",
    "#subdata[match[id]]=subdata[match[id]].drop(columns=[\"point_victor\"])\n",
    "#features=subdata[match[id]].iloc[:,4:]\n",
    "#print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# 数据划分\n",
    "X_train={}\n",
    "X_test={}\n",
    "y_train={}\n",
    "y_test={}\n",
    "for id in range(len(match)):\n",
    "  X_train[match[id]], X_test[match[id]], y_train[match[id]], y_test[match[id]] = train_test_split(features[match[id]], target[match[id]], test_size=0.3, random_state=42)\n",
    "\n",
    "#print(X_train.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 数据标准化\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 模型训练\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "#print(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 模型预测\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 模型评估\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'模型准确率: {accuracy}')\n",
    "\n",
    "# 可以考虑输出特征重要性，帮助理解哪些因素对预测结果影响较大\n",
    "importances = clf.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "feature_importances = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
    "print(feature_importances)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
