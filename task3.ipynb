{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43.78409, 40.59322, 39.725663, 37.31208, 39.0, 36.600706, 37.048543, 34.923076, 38.327956, 41.123674, 35.568493, 38.151639, 34.312252, 33.364197, 34.539772, 41.554794, 41.755, 41.395833, 34.773584, 34.866141, 40.021276, 35.890173, 31.554054, 37.925, 40.95238, 33.7, 37.491124, 42.31088, 37.605633, 47.819767, 47.070707]\n"
     ]
    }
   ],
   "source": [
    "#from itertools import chain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from re import T\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "#from math import sqrt\n",
    "#import markovify\n",
    "id=0\n",
    "\n",
    "############ load and digitize the data\n",
    "data=pd.read_csv('/home/ebotian/MCM/tennis2.csv')\n",
    "\n",
    "############\n",
    "\n",
    "############# pre-process the data\n",
    "    # fill nan with 0, and replace AD with 50\n",
    "def pre_process(data):\n",
    "    data = pd.get_dummies(data, columns=['winner_shot_type','serve_width','serve_depth','return_depth'])\n",
    "    data = data.fillna(0)\n",
    "    data = data.replace('AD', 50.0)\n",
    "    #data['point_victor']=data[\"point_victor\"].replace(2,0)\n",
    "    #print(data.iloc[:,15])\n",
    "    ###################################\n",
    "    #for predict:\n",
    "    #conditions = [\n",
    "    #    (data['server'] == 1) & (data['point_victor'] == 1),\n",
    "    #    (data['server'] == 1) & (data['point_victor'] == 2),\n",
    "    #    (data['server'] == 2) & (data['point_victor'] == 2),\n",
    "    #    (data['server'] == 2) & (data['point_victor'] == 1)\n",
    "    #]\n",
    "    #choices = ['P1_serve_win', 'P1_serve_lose', 'P2_serve_win', 'P2_serve_lose']\n",
    "    #data['state'] = np.select(conditions, choices, default='unknown')\n",
    "\n",
    "    # split the data into different match\n",
    "    grouped = dict(tuple(data.groupby(data['match_id'].ne(data['match_id'].shift()).cumsum())))\n",
    "\n",
    "    # Rename the subdata\n",
    "    subdata = {df['match_id'].iloc[0]: df for _, df in grouped.items()}\n",
    "\n",
    "    # Create a new dataset from the first column, excluding duplicates\n",
    "    match = pd.DataFrame(data.iloc[:, 0].drop_duplicates()).iloc[:,0].tolist()\n",
    "\n",
    "\n",
    "    return subdata,match\n",
    "\n",
    "subdata,match=pre_process(data)\n",
    "#print(subdata[match[1]])\n",
    "##############\n",
    "\n",
    "def process_all_ids(subdata):\n",
    "    features={}\n",
    "    target={}\n",
    "    for id in range(len(match)):\n",
    "        #index_array = subdata[match[id]][subdata[match[id]]['server'] == 2].index.values\n",
    "#        subdata[match[id]].loc[index_array, 'point_victor'] = 1 - subdata[match[id]].loc[index_array, 'point_victor']\n",
    "        target[match[id]]=pd.DataFrame(subdata[match[id]][\"point_victor\"])\n",
    "        # Add the \"elapsed_time\" column to the \"target\" DataFrame\n",
    "        subdata[match[id]]['elapsed_time'] = pd.to_timedelta(subdata[match[id]]['elapsed_time'])\n",
    "        #target[match[id]].insert(0, 'elapsed_time', subdata[match[id]]['elapsed_time'])\n",
    "        #target['elapsed_time'] = target['elapsed_time'].dt.total_seconds()\n",
    "        features[match[id]]=subdata[match[id]].drop(columns=[\"point_victor\"]).iloc[:,4:]\n",
    "        subdata[match[id]]=subdata[match[id]].drop(columns=[\"point_victor\"])\n",
    "\n",
    "    return target,features,subdata\n",
    "\n",
    "# Replace with your actual match ids\n",
    "target,features,subdata=process_all_ids(subdata)\n",
    "#print(subdata[match[1]])\n",
    "##############\n",
    "##############\n",
    "#invert the victor when the server is 2 to get server_victor\n",
    "#and after the prediction, we invert the victor again\n",
    "##############\n",
    "# Calculate the time difference between consecutive rows\n",
    "def get_average_interval(id, subdata):\n",
    "\n",
    "    # Calculate the time difference between consecutive rows\n",
    "    subdata[match[id]]['time_diff'] = subdata[match[id]]['elapsed_time'].diff()\n",
    "\n",
    "    # Calculate the 5th and 95th percentiles\n",
    "    lower_threshold = subdata[match[id]]['time_diff'].quantile(0.05)\n",
    "    upper_threshold = subdata[match[id]]['time_diff'].quantile(0.95)\n",
    "\n",
    "    # Exclude the top 5% and bottom 5% of periods\n",
    "    filtered_diff = subdata[match[id]]['time_diff'][(subdata[match[id]]['time_diff'] > lower_threshold) & (subdata[match[id]]['time_diff'] < upper_threshold)]\n",
    "\n",
    "    # Calculate the average of the remaining intervals\n",
    "    average_interval = filtered_diff.mean()\n",
    "    return average_interval.total_seconds()\n",
    "\n",
    "average_interval_time=[]\n",
    "\n",
    "for id in range(len(match)):\n",
    "    average_interval_time.append(get_average_interval(id, subdata))\n",
    "\n",
    "#print(average_interval_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ add features\n",
    "add_feature=[]\n",
    "add_feature.append(\"p1_net_pt_probability\")\n",
    "add_feature.append(\"p2_net_pt_probability\")\n",
    "add_feature.append(\"p1_break_pt_probability\")\n",
    "add_feature.append(\"p2_break_pt_probability\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## defining the new features\n",
    "#for id in range(len(match)):\n",
    "#  for cnt in range(len(add_feature)):\n",
    "#    features[match[id]][add_feature[cnt]] = subdata[match[id]]['p1_net_pt']/ subdata[match[id]][\"p1_net_pt_won\"]\n",
    "#    features[match[id]][add_feature[cnt]] = subdata[match[id]]['p2_net_pt']/ subdata[match[id]][\"p2_net_pt_won\"]\n",
    "#    features[match[id]][add_feature[cnt]] = subdata[match[id]][\"p1_break_pt\"]/ subdata[match[id]][\"p1_break_pt_won\"]\n",
    "#    features[match[id]][add_feature[cnt]] = subdata[match[id]][\"p2_break_pt\"]/ subdata[match[id]][\"p2_break_pt_won\"]\n",
    "\n",
    "\n",
    "#data[add_feature_1] = data['p1_games'] - data['p2_games']\n",
    "#data[add_feature_2] = data['p1_winner'] / (data['p1_winner'] + data['p2_winner'])\n",
    "#data[add_feature_3] = data['p2_winner'] / (data['p1_winner'] + data['p2_winner'])\n",
    "#data[add_feature_4] = data['p1_break_pt_won'] / (data['p1_break_pt_won'] + data['p1_break_pt_missed'])\n",
    "#data[add_feature_5] = data['p2_break_pt_won'] / (data['p2_break_pt_won'] + data['p2_break_pt_missed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into features and target\n",
    "#target=pd.DataFrame(subdata[match[id]][\"point_victor\"])\n",
    "#\n",
    "#subdata[match[id]]=subdata[match[id]].drop(columns=[\"point_victor\"])\n",
    "#features=subdata[match[id]].iloc[:,4:]\n",
    "#print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# 数据划分\n",
    "X_train={}\n",
    "X_test={}\n",
    "y_train={}\n",
    "y_test={}\n",
    "for id in range(len(match)):\n",
    "  X_train[match[id]], X_test[match[id]], y_train[match[id]], y_test[match[id]] = train_test_split(features[match[id]], target[match[id]], test_size=0.3, random_state=42)\n",
    "\n",
    "#print(X_train.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233\n",
      "233\n",
      "233\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "#print(len(X_train[match[id]]))\n",
    "#print(len(y_train[match[id]]))\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train[match[id]].values)\n",
    "#print(len(X_train_scaled))\n",
    "#print(X_train[match[id]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#print(X_train[match[0]].values.reshape(-1, 1))\n",
    "classifier={}\n",
    "X_test_scaled={}\n",
    "\n",
    "for id in range(len(match)):\n",
    "  scaler = StandardScaler()\n",
    "  X_train_scaled = scaler.fit_transform(X_train[match[id]].values)\n",
    "  X_test_scaled[match[id]] = scaler.transform(X_test[match[id]].values)\n",
    "  classifier[match[id]] = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "  classifier[match[id]].fit(X_train_scaled, y_train[match[id]].values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 模型预测\n",
    "y_pred={}\n",
    "for id in range(len(match)):\n",
    "  y_pred[match[id]] = classifier[match[id]].predict(X_test_scaled[match[id]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set_no                NaN\n",
      "game_no               NaN\n",
      "point_no              NaN\n",
      "p1_sets               NaN\n",
      "p2_sets               NaN\n",
      "p1_games              NaN\n",
      "p2_games              NaN\n",
      "p1_score              NaN\n",
      "p2_score              NaN\n",
      "server                NaN\n",
      "serve_no              NaN\n",
      "p1_points_won         NaN\n",
      "p2_points_won         NaN\n",
      "game_victor           NaN\n",
      "set_victor            NaN\n",
      "p1_ace                NaN\n",
      "p2_ace                NaN\n",
      "p1_winner             NaN\n",
      "p2_winner             NaN\n",
      "p1_double_fault       NaN\n",
      "p2_double_fault       NaN\n",
      "p1_unf_err            NaN\n",
      "p2_unf_err            NaN\n",
      "p1_net_pt             NaN\n",
      "p2_net_pt             NaN\n",
      "p1_net_pt_won         NaN\n",
      "p2_net_pt_won         NaN\n",
      "p1_break_pt           NaN\n",
      "p2_break_pt           NaN\n",
      "p1_break_pt_won       NaN\n",
      "p2_break_pt_won       NaN\n",
      "p1_break_pt_missed    NaN\n",
      "p2_break_pt_missed    NaN\n",
      "p1_distance_run       NaN\n",
      "p2_distance_run       NaN\n",
      "rally_count           NaN\n",
      "speed_mph             NaN\n",
      "winner_shot_type_0    NaN\n",
      "winner_shot_type_B    NaN\n",
      "winner_shot_type_F    NaN\n",
      "serve_width_B         NaN\n",
      "serve_width_BC        NaN\n",
      "serve_width_BW        NaN\n",
      "serve_width_C         NaN\n",
      "serve_width_W         NaN\n",
      "serve_depth_CTL       NaN\n",
      "serve_depth_NCTL      NaN\n",
      "return_depth_D        NaN\n",
      "return_depth_ND       NaN\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy={}\n",
    "importances={}\n",
    "for id in range(len(match)):\n",
    "  accuracy[match[id]] = accuracy_score(y_test[match[id]], y_pred[match[id]])\n",
    "  importances[match[id]] = classifier[match[id]].feature_importances_\n",
    "  feature_names = X_train[match[id]].columns\n",
    "  feature_importances = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
    "\n",
    "print(feature_importances)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
