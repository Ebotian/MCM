{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average interval (excluding top 5% and bottom 5%): 0 days 00:00:43.784090909\n",
      "RMSE: 0.6411794687223782\n",
      "Number of same values: 53\n",
      "Percentage: 58.88888888888889%\n",
      "Percentage of Markov: 53.333333333333336%\n",
      "Percentage of ARIMA: 52.22222222222223%\n",
      "Percentage of SARIMA: 58.88888888888889%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "############ load and digitize the data\n",
    "data=pd.read_csv('/home/ebotian/MCM/tennis.csv')\n",
    "data = pd.get_dummies(data, columns=['winner_shot_type','serve_width','serve_depth','return_depth'])\n",
    "############\n",
    "\n",
    "############# pre-process the data\n",
    "# fill nan with 0, and replace AD with 50\n",
    "data = data.fillna(0)\n",
    "data = data.replace('AD', 50.0)\n",
    "data['point_victor']=data[\"point_victor\"].replace(2,0)\n",
    "#print(data.iloc[:,15])\n",
    "\n",
    "# split the data into different match\n",
    "grouped = dict(tuple(data.groupby(data['match_id'].ne(data['match_id'].shift()).cumsum())))\n",
    "\n",
    "# Rename the subdata\n",
    "subdata = {df['match_id'].iloc[0]: df for _, df in grouped.items()}\n",
    "\n",
    "# Create a new dataset from the first column, excluding duplicates\n",
    "match = pd.DataFrame(data.iloc[:, 0].drop_duplicates()).iloc[:,0].tolist()\n",
    "#print(match_id[0])\n",
    "##############\n",
    "id=0\n",
    "##############\n",
    "##############\n",
    "#invert the victor when the server is 2 to get server_victor\n",
    "#and after the prediction, we invert the victor again\n",
    "# Get the index array\n",
    "index_array = subdata[match[id]][subdata[match[id]]['server'] == 2].index.values\n",
    "\n",
    "# Invert the values in the \"point_victor\" column for the specified rows\n",
    "subdata[match[id]].loc[index_array, 'point_victor'] = 1 - subdata[match[id]].loc[index_array, 'point_victor']\n",
    "#print(index_array)\n",
    "##############\n",
    "# Calculate the time difference between consecutive rows\n",
    "def get_average_interval(id, subdata):\n",
    "    # Convert the timestamp column to datetime format\n",
    "    subdata[match[id]]['elapsed_time'] = pd.to_timedelta(subdata[match[id]]['elapsed_time'])\n",
    "\n",
    "    # Calculate the time difference between consecutive rows\n",
    "    subdata[match[id]]['time_diff'] = subdata[match[id]]['elapsed_time'].diff()\n",
    "\n",
    "    # Calculate the 5th and 95th percentiles\n",
    "    lower_threshold = subdata[match[id]]['time_diff'].quantile(0.05)\n",
    "    upper_threshold = subdata[match[id]]['time_diff'].quantile(0.95)\n",
    "\n",
    "    # Exclude the top 5% and bottom 5% of periods\n",
    "    filtered_diff = subdata[match[id]]['time_diff'][(subdata[match[id]]['time_diff'] > lower_threshold) & (subdata[match[id]]['time_diff'] < upper_threshold)]\n",
    "\n",
    "    # Calculate the average of the remaining intervals\n",
    "    average_interval = filtered_diff.mean()\n",
    "\n",
    "    return average_interval\n",
    "\n",
    "\n",
    "########## defining the new features\n",
    "\n",
    "#subdata[match_id[0]][add_feature[0]] = subdata[match_id[0]]['p1_games'] - subdata[match_id[0]]['p2_games']\n",
    "# split the data into features and target\n",
    "target=pd.DataFrame(subdata[match[id]][\"point_victor\"])\n",
    "# Add the \"elapsed_time\" column to the \"target\" DataFrame\n",
    "target.insert(0, 'elapsed_time', subdata[match[id]]['elapsed_time'])\n",
    "target['elapsed_time'] = target['elapsed_time'].dt.total_seconds()\n",
    "\n",
    "subdata[match[id]]=subdata[match[id]].drop(columns=[\"point_victor\"])\n",
    "features=subdata[match[id]].iloc[:,4:]\n",
    "#print(target)\n",
    "from re import T\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Split data into training set and test set\n",
    "train_size = int(len(target) * 0.7)\n",
    "train, test = target[0:train_size], target[train_size:len(target)]\n",
    "train=train.astype(float)\n",
    "#print(train)\n",
    "#print(test)\n",
    "\n",
    "#find index\n",
    "# Get the indices in train that are in index_array\n",
    "train_indices = np.intersect1d(train.index.values, index_array)\n",
    "\n",
    "# Get the indices in test that are in index_array\n",
    "test_indices = np.intersect1d(test.index.values, index_array)\n",
    "\n",
    "train_array_indices = train_indices - min(train.index.values)\n",
    "test_array_indices = test_indices - min(test.index.values)\n",
    "\n",
    "\n",
    "#Markov Chain\n",
    "from pydtmc import MarkovChain\n",
    "\n",
    "def train_and_predict_markov(train_data, test_data, test_array_indices):\n",
    "    # Calculate the transition matrix\n",
    "    transition_matrix = np.zeros((2, 2))\n",
    "    for i in range(len(train_data) - 1):\n",
    "        transition_matrix[int(train_data[i]), int(train_data[i+1])] += 1\n",
    "    transition_matrix /= transition_matrix.sum(axis=1, keepdims=True)\n",
    "\n",
    "    # Create the Markov Chain\n",
    "    mc = MarkovChain(transition_matrix, ['0', '1'])\n",
    "\n",
    "    # Function to predict the next state\n",
    "    def predict_next_state(mc, current_state):\n",
    "        return np.random.choice(mc.states, p=mc.p[int(current_state)])\n",
    "\n",
    "    # Get the last state from the training data\n",
    "    last_state_train = train_data[-1]\n",
    "\n",
    "    # Generate the first prediction from the last state of the training data\n",
    "    first_prediction = predict_next_state(mc, last_state_train)\n",
    "\n",
    "    # Generate the rest of the predictions from the test data\n",
    "    predictions_markov = [predict_next_state(mc, state) for state in test_data[:-1]]\n",
    "\n",
    "    # Insert the first prediction at the beginning of the predictions list\n",
    "    predictions_markov.insert(0, first_prediction)\n",
    "    predictions_markov = np.array(predictions_markov).astype(float)\n",
    "\n",
    "    # Invert the values in predictions and test_data for the specified indices\n",
    "    predictions_markov[test_array_indices] = 1 - predictions_markov[test_array_indices]\n",
    "    test_data[test_array_indices] = 1 - test_data[test_array_indices]\n",
    "\n",
    "    # Calculate the percentage of correct predictions\n",
    "    same_values_markov = (test_data == predictions_markov).sum()\n",
    "    percentage_markov = same_values_markov / len(test_data) * 100\n",
    "\n",
    "    return predictions_markov, percentage_markov, mc\n",
    "\n",
    "\n",
    "\n",
    "# ARIMA\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "def train_and_predict_arima(train_data, test_data, test_array_indices):\n",
    "    # Train the ARIMA model\n",
    "    model_arima = ARIMA(train_data, order=(5,1,0))\n",
    "    model_arima_fit = model_arima.fit()\n",
    "\n",
    "    # Make predictions\n",
    "    predictions_arima = model_arima_fit.forecast(steps=len(test_data))\n",
    "\n",
    "    # Invert the predictions for the specified indices\n",
    "    predictions_arima[test_array_indices] = 1 - predictions_arima[test_array_indices]\n",
    "    test_data[test_array_indices] = 1 - test_data[test_array_indices]\n",
    "\n",
    "    # Convert the ARIMA predictions to binary\n",
    "    predictions_arima_binary = np.where(predictions_arima > 0.5, 1, 0)\n",
    "\n",
    "    # Calculate the percentage of correct predictions\n",
    "    same_values_arima = (test_data == predictions_arima_binary).sum()\n",
    "    percentage_arima = same_values_arima / len(test_data) * 100\n",
    "\n",
    "    return predictions_arima, percentage_arima, model_arima_fit\n",
    "\n",
    "train_data = train[\"point_victor\"].values\n",
    "test_data = test[\"point_victor\"].values\n",
    "\n",
    "\n",
    "\n",
    "# SARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "def train_and_predict_sarima(train_data, test_data, test_array_indices):\n",
    "    # Train the SARIMA model\n",
    "    model_sarima = SARIMAX(train_data, order=(1, 1, 1), seasonal_order=(0, 0, 0, 0))\n",
    "    model_sarima_fit = model_sarima.fit(disp=0)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions_sarima = model_sarima_fit.forecast(steps=len(test_data))\n",
    "    predictions_sarima = np.array(predictions_sarima)\n",
    "\n",
    "    # Invert the predictions for the specified indices\n",
    "    predictions_sarima[test_array_indices] = 1 - predictions_sarima[test_array_indices]\n",
    "    test_data[test_array_indices] = 1 - test_data[test_array_indices]\n",
    "\n",
    "    # Convert the SARIMA predictions to binary\n",
    "    predictions_sarima_binary = np.where(predictions_sarima > 0.5, 1, 0)\n",
    "\n",
    "    # Calculate the percentage of correct predictions\n",
    "    same_values_sarima = (test_data == predictions_sarima_binary).sum()\n",
    "    percentage_sarima = same_values_sarima / len(test_data) * 100\n",
    "\n",
    "    return predictions_sarima, percentage_sarima, model_sarima_fit\n",
    "\n",
    "train_data = train[\"point_victor\"].values\n",
    "test_data = test[\"point_victor\"].values\n",
    "predictions_markov, percentage_markov,mc = train_and_predict_markov(train_data, test_data, test_array_indices)\n",
    "predictions_arima, percentage_arima,model_arima_fit = train_and_predict_arima(train_data, test_data, test_array_indices)\n",
    "predictions_sarima, percentage_sarima,model_sarima_fit = train_and_predict_sarima(train_data, test_data, test_array_indices)\n",
    "\n",
    "# Calculate the weights\n",
    "total_weight=(abs(percentage_markov / 100 - 0.5) + abs(percentage_arima / 100 - 0.5) + abs(percentage_sarima / 100 - 0.5))\n",
    "weight_markov = abs(percentage_markov / 100 - 0.5) / total_weight\n",
    "weight_arima = abs(percentage_arima / 100 - 0.5) / total_weight\n",
    "weight_sarima = abs(percentage_sarima / 100 - 0.5)/ total_weight\n",
    "# Reverse the predictions if the accuracy is less than 50%\n",
    "if percentage_markov < 50:\n",
    "    predictions_markov = [1.0 - p for p in predictions_markov]\n",
    "if percentage_arima < 50:\n",
    "    predictions_arima = [1.0 - p for p in predictions_arima]\n",
    "if percentage_sarima < 50:\n",
    "    predictions_sarima = [1.0 - p for p in predictions_sarima]\n",
    "\n",
    "#print(predictions_markov)\n",
    "# Convert lists to numpy arrays\n",
    "# Convert lists to numpy arrays\n",
    "predictions_markov = np.array(predictions_markov)\n",
    "predictions_arima = np.array(predictions_arima)\n",
    "predictions_sarima = np.array(predictions_sarima)\n",
    "\n",
    "# Calculate the combined predictions\n",
    "combined_predictions = weight_markov * predictions_markov + weight_arima * predictions_arima + weight_sarima * predictions_sarima\n",
    "\n",
    "# Convert the combined predictions to binary\n",
    "combined_predictions_binary = np.where(combined_predictions > 0.5, 1, 0)\n",
    "\n",
    "# Calculate the RMSE\n",
    "rmse_combined = sqrt(mean_squared_error(test_data, combined_predictions_binary))\n",
    "\n",
    "# Calculate the number of same values\n",
    "same_values_combined = (test_data == combined_predictions_binary).sum()\n",
    "\n",
    "# Calculate the percentage\n",
    "percentage_combined = same_values_combined / len(test_data) * 100\n",
    "\n",
    "print(f\"RMSE: {rmse_combined}\")\n",
    "print(f\"Number of same values: {same_values_combined}\")\n",
    "print(f\"Percentage: {percentage_combined}%\")\n",
    "\n",
    "#list all three alone percentage\n",
    "print(f\"Percentage of Markov: {percentage_markov}%\")\n",
    "print(f\"Percentage of ARIMA: {percentage_arima}%\")\n",
    "print(f\"Percentage of SARIMA: {percentage_sarima}%\")\n",
    "# Print RMSE values\n",
    "\n",
    "#weight calculate\n",
    "def calculate_weights(percentage_markov, percentage_arima, percentage_sarima):\n",
    "    # Calculate the total weight\n",
    "    total_weight = (abs(percentage_markov / 100 - 0.5) + abs(percentage_arima / 100 - 0.5) + abs(percentage_sarima / 100 - 0.5))\n",
    "\n",
    "    # Calculate the individual weights\n",
    "    weight_markov = abs(percentage_markov / 100 - 0.5) / total_weight\n",
    "    weight_arima = abs(percentage_arima / 100 - 0.5) / total_weight\n",
    "    weight_sarima = abs(percentage_sarima / 100 - 0.5) / total_weight\n",
    "\n",
    "    return weight_markov, weight_arima, weight_sarima\n",
    "\n",
    "def reverse_predictions(predictions, percentage):\n",
    "    # Reverse the predictions if the accuracy is less than 50%\n",
    "    if percentage < 50:\n",
    "        predictions = [1.0 - p for p in predictions]\n",
    "    return predictions\n",
    "\n",
    "def find_best_train_size(start, end, step,id):\n",
    "    best_percentage = 0\n",
    "    best_train_size = 0\n",
    "\n",
    "    for train_size in np.arange(start, end, step):\n",
    "        # Split the data into training and test sets\n",
    "        train_data = subdata[match[id]][:int(len(data) * train_size)].values\n",
    "        test_data = subdata[match[id]][int(len(data) * train_size):]\n",
    "        train_data = train[\"point_victor\"].values\n",
    "        test_data = test[\"point_victor\"].values\n",
    "\n",
    "        # Train the models and make predictions\n",
    "        # (replace this with your actual model training and prediction code)\n",
    "        predictions_markov, percentage_markov,mc = train_and_predict_markov(train_data, test_data, test_array_indices)\n",
    "        predictions_arima, percentage_arima,model_arima_fit = train_and_predict_arima(train_data, test_data, test_array_indices)\n",
    "        predictions_sarima, percentage_sarima,model_sarima_fit = train_and_predict_sarima(train_data, test_data, test_array_indices)\n",
    "\n",
    "        # Calculate the weights\n",
    "        weight_markov, weight_arima, weight_sarima = calculate_weights(percentage_markov, percentage_arima, percentage_sarima)\n",
    "\n",
    "        # Reverse the predictions if the accuracy is less than 50%\n",
    "        predictions_markov = reverse_predictions(predictions_markov, percentage_markov)\n",
    "        predictions_arima = reverse_predictions(predictions_arima, percentage_arima)\n",
    "        predictions_sarima = reverse_predictions(predictions_sarima, percentage_sarima)\n",
    "\n",
    "        # Convert lists to numpy arrays\n",
    "        predictions_markov = np.array(predictions_markov)\n",
    "        predictions_arima = np.array(predictions_arima)\n",
    "        predictions_sarima = np.array(predictions_sarima)\n",
    "\n",
    "        # Calculate the combined predictions\n",
    "        combined_predictions = weight_markov * predictions_markov + weight_arima * predictions_arima + weight_sarima * predictions_sarima\n",
    "\n",
    "        # Convert the combined predictions to binary\n",
    "        combined_predictions_binary = np.where(combined_predictions > 0.5, 1, 0)\n",
    "\n",
    "        # Calculate the percentage\n",
    "        same_values_combined = (test_data == combined_predictions_binary).sum()\n",
    "        percentage_combined = same_values_combined / len(test_data) * 100\n",
    "\n",
    "        # Update best_percentage and best_train_size if this is the highest percentage so far\n",
    "        if percentage_combined > best_percentage:\n",
    "            best_percentage = percentage_combined\n",
    "            best_train_size = train_size\n",
    "\n",
    "    return best_train_size, best_percentage,mc,model_arima_fit,model_sarima_fit\n",
    "\n",
    "start = 0.5  # start of the train size range\n",
    "end = 0.98  # end of the train size range\n",
    "step = 0.02  # step size for the train size range\n",
    "id = 0  # replace this with the actual id\n",
    "\n",
    "#best_train_size, best_percentage = find_best_train_size(start, end, step, id)\n",
    "\n",
    "#print(f\"Best train size: {best_train_size}, Best percentage: {best_percentage}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0, Best train size: 0.9400000000000004, Best percentage: 61.111111111111114%\n",
      "Index: 1, Best train size: 0.9400000000000004, Best percentage: 61.111111111111114%\n",
      "Index: 2, Best train size: 0.5, Best percentage: 58.88888888888889%\n",
      "Index: 3, Best train size: 0.5, Best percentage: 60.0%\n",
      "Index: 4, Best train size: 0.8800000000000003, Best percentage: 60.0%\n",
      "Index: 5, Best train size: 0.5800000000000001, Best percentage: 58.88888888888889%\n",
      "Index: 6, Best train size: 0.52, Best percentage: 58.88888888888889%\n",
      "Index: 7, Best train size: 0.54, Best percentage: 63.33333333333333%\n",
      "Index: 8, Best train size: 0.7400000000000002, Best percentage: 62.22222222222222%\n",
      "Index: 9, Best train size: 0.56, Best percentage: 62.22222222222222%\n",
      "Index: 10, Best train size: 0.5800000000000001, Best percentage: 58.88888888888889%\n",
      "Index: 11, Best train size: 0.54, Best percentage: 58.88888888888889%\n",
      "Index: 12, Best train size: 0.52, Best percentage: 58.88888888888889%\n",
      "Index: 13, Best train size: 0.5, Best percentage: 58.88888888888889%\n",
      "Index: 14, Best train size: 0.8400000000000003, Best percentage: 70.0%\n",
      "Index: 15, Best train size: 0.7600000000000002, Best percentage: 63.33333333333333%\n",
      "Index: 16, Best train size: 0.5, Best percentage: 58.88888888888889%\n",
      "Index: 17, Best train size: 0.5, Best percentage: 58.88888888888889%\n",
      "Index: 18, Best train size: 0.9600000000000004, Best percentage: 64.44444444444444%\n",
      "Index: 19, Best train size: 0.6600000000000001, Best percentage: 63.33333333333333%\n",
      "Index: 20, Best train size: 0.54, Best percentage: 60.0%\n",
      "Index: 21, Best train size: 0.54, Best percentage: 60.0%\n",
      "Index: 22, Best train size: 0.9400000000000004, Best percentage: 61.111111111111114%\n",
      "Index: 23, Best train size: 0.5, Best percentage: 58.88888888888889%\n",
      "Index: 24, Best train size: 0.5, Best percentage: 58.88888888888889%\n",
      "Index: 25, Best train size: 0.5, Best percentage: 58.88888888888889%\n",
      "Index: 26, Best train size: 0.5, Best percentage: 58.88888888888889%\n",
      "Index: 27, Best train size: 0.6200000000000001, Best percentage: 62.22222222222222%\n",
      "Index: 28, Best train size: 0.5, Best percentage: 58.88888888888889%\n",
      "Index: 29, Best train size: 0.6200000000000001, Best percentage: 62.22222222222222%\n",
      "Index: 30, Best train size: 0.7800000000000002, Best percentage: 60.0%\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def find_best_train_size_async(start, end, step, id):\n",
    "    # Wrap the synchronous function in a executor\n",
    "    loop = asyncio.get_event_loop()\n",
    "    return await loop.run_in_executor(None, find_best_train_size, start, end, step, id)\n",
    "\n",
    "# Initialize variables to store the best train size and percentage for each index\n",
    "best_train_sizes = {}\n",
    "best_percentages = {}\n",
    "\n",
    "# Create a list to hold all the tasks\n",
    "tasks = []\n",
    "\n",
    "# Loop over all indices in the match list\n",
    "for id in range(len(match)):\n",
    "    # Create a task for this index and add it to the list of tasks\n",
    "    task = asyncio.ensure_future(find_best_train_size_async(start, end, step, id))\n",
    "    tasks.append(task)\n",
    "\n",
    "# Run all the tasks concurrently\n",
    "results = asyncio.get_event_loop().run_until_complete(asyncio.gather(*tasks))\n",
    "\n",
    "# Store the best train size and percentage for each index\n",
    "for id in range(len(match)):\n",
    "    best_train_sizes[id], best_percentages[id] = results[id]\n",
    "\n",
    "# Print the best train size and percentage for each index\n",
    "for id in range(len(match)):\n",
    "    print(f\"Index: {id}, Best train size: {best_train_sizes[id]}, Best percentage: {best_percentages[id]}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
