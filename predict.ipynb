{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "############ load and digitize the data\n",
    "data=pd.read_csv('/home/ebotian/MCM/tennis.csv')\n",
    "data = pd.get_dummies(data, columns=['winner_shot_type','serve_width','serve_depth','return_depth'])\n",
    "############\n",
    "\n",
    "############# pre-process the data\n",
    "# fill nan with 0, and replace AD with 50\n",
    "data = data.fillna(0)\n",
    "data = data.replace('AD', 50.0)\n",
    "data['point_victor']=data[\"point_victor\"].replace(2,0)\n",
    "#print(data.iloc[:,15])\n",
    "\n",
    "# split the data into different match\n",
    "grouped = dict(tuple(data.groupby(data['match_id'].ne(data['match_id'].shift()).cumsum())))\n",
    "\n",
    "# Rename the subdata\n",
    "subdata = {df['match_id'].iloc[0]: df for _, df in grouped.items()}\n",
    "\n",
    "# Create a new dataset from the first column, excluding duplicates\n",
    "match = pd.DataFrame(data.iloc[:, 0].drop_duplicates()).iloc[:,0].tolist()\n",
    "#print(match_id[0])\n",
    "##############\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the id of the match\n",
    "id=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average interval (excluding top 5% and bottom 5%): 0 days 00:00:43.784090909\n"
     ]
    }
   ],
   "source": [
    "# Calculate the time difference between consecutive rows\n",
    "\n",
    "# Convert the timestamp column to datetime format\n",
    "subdata[match[id]]['elapsed_time'] = pd.to_timedelta(subdata[match[id]]['elapsed_time'])\n",
    "\n",
    "# Calculate the time difference between consecutive rows\n",
    "subdata[match[id]]['time_diff'] = subdata[match[id]]['elapsed_time'].diff()\n",
    "\n",
    "# Calculate the 5th and 95th percentiles\n",
    "lower_threshold = subdata[match[id]]['time_diff'].quantile(0.05)\n",
    "upper_threshold = subdata[match[id]]['time_diff'].quantile(0.95)\n",
    "\n",
    "# Exclude the top 5% and bottom 5% of periods\n",
    "filtered_diff = subdata[match[id]]['time_diff'][(subdata[match[id]]['time_diff'] > lower_threshold) & (subdata[match[id]]['time_diff'] < upper_threshold)]\n",
    "\n",
    "# Calculate the average of the remaining intervals\n",
    "average_interval = filtered_diff.mean()\n",
    "\n",
    "print(f'Average interval (excluding top 5% and bottom 5%): {average_interval}')\n",
    "\n",
    "# Convert the time differences to integer seconds\n",
    "subdata[match[id]]['time_diff'] = subdata[match[id]]['time_diff'].dt.total_seconds()\n",
    "\n",
    "# Replace 'NaT' values with 0\n",
    "subdata[match[id]]['time_diff'] = subdata[match[id]]['time_diff'].fillna(0).astype(int)\n",
    "#fill nan with 0\n",
    "#subdata[match[id]]['time_diff'] = subdata[match[id]]['time_diff'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ add features\n",
    "add_feature=[\"score_diff\"]\n",
    "\n",
    "########## defining the new features\n",
    "\n",
    "#subdata[match_id[0]][add_feature[0]] = subdata[match_id[0]]['p1_games'] - subdata[match_id[0]]['p2_games']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into features and target\n",
    "target=pd.DataFrame(subdata[match[id]][\"point_victor\"])\n",
    "# Add the \"elapsed_time\" column to the \"target\" DataFrame\n",
    "target.insert(0, 'elapsed_time', subdata[match[id]]['elapsed_time'])\n",
    "target['elapsed_time'] = target['elapsed_time'].dt.total_seconds()\n",
    "\n",
    "subdata[match[id]]=subdata[match[id]].drop(columns=[\"point_victor\"])\n",
    "features=subdata[match[id]].iloc[:,4:]\n",
    "#print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     elapsed_time  point_victor\n",
      "0             0.0           0.0\n",
      "1            38.0           1.0\n",
      "2            61.0           0.0\n",
      "3            91.0           1.0\n",
      "4           141.0           1.0\n",
      "..            ...           ...\n",
      "235       11176.0           1.0\n",
      "236       11229.0           0.0\n",
      "237       11258.0           0.0\n",
      "238       11342.0           1.0\n",
      "239       11395.0           0.0\n",
      "\n",
      "[240 rows x 2 columns]\n",
      "     elapsed_time  point_victor\n",
      "240       11427.0             0\n",
      "241       11554.0             1\n",
      "242       11571.0             0\n",
      "243       11592.0             0\n",
      "244       11627.0             0\n",
      "245       11647.0             1\n",
      "246       11684.0             1\n",
      "247       11712.0             1\n",
      "248       11753.0             1\n",
      "249       11818.0             0\n",
      "250       11842.0             1\n",
      "251       11875.0             0\n",
      "252       11924.0             0\n",
      "253       11953.0             1\n",
      "254       11986.0             1\n",
      "255       12021.0             0\n",
      "256       12054.0             0\n",
      "257       12178.0             0\n",
      "258       12235.0             1\n",
      "259       12303.0             1\n",
      "260       12328.0             1\n",
      "261       12364.0             1\n",
      "262       12422.0             1\n",
      "263       12488.0             1\n",
      "264       12536.0             0\n",
      "265       12561.0             1\n",
      "266       12594.0             0\n",
      "267       12631.0             1\n",
      "268       12747.0             1\n",
      "269       12781.0             1\n",
      "270       12825.0             0\n",
      "271       12864.0             1\n",
      "272       12893.0             1\n",
      "273       12932.0             0\n",
      "274       12954.0             1\n",
      "275       12986.0             0\n",
      "276       13012.0             0\n",
      "277       13041.0             0\n",
      "278       13172.0             1\n",
      "279       13214.0             1\n",
      "280       13260.0             0\n",
      "281       13292.0             1\n",
      "282       13336.0             0\n",
      "283       13368.0             0\n",
      "284       13429.0             1\n",
      "285       13463.0             1\n",
      "286       13510.0             0\n",
      "287       13548.0             1\n",
      "288       13588.0             0\n",
      "289       13635.0             1\n",
      "290       13671.0             1\n",
      "291       13738.0             1\n",
      "292       13855.0             0\n",
      "293       13912.0             1\n",
      "294       13950.0             1\n",
      "295       14005.0             0\n",
      "296       14041.0             0\n",
      "297       14102.0             1\n",
      "298       14134.0             1\n",
      "299       14171.0             1\n"
     ]
    }
   ],
   "source": [
    "from re import T\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Split data into training set and test set\n",
    "train_size = int(len(target) * 0.7)\n",
    "train, test = target[0:train_size], target[train_size:len(target)]\n",
    "train=train.astype(float)\n",
    "print(train)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7187952884282609\n",
      "Number of same values: 29\n",
      "Percentage: 48.333333333333336%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pydtmc import MarkovChain\n",
    "\n",
    "# Prepare the data\n",
    "train_data = train[\"point_victor\"].values\n",
    "\n",
    "# Calculate the transition matrix\n",
    "transition_matrix = np.zeros((2, 2))\n",
    "for i in range(len(train_data) - 1):\n",
    "    transition_matrix[int(train_data[i]), int(train_data[i+1])] += 1\n",
    "transition_matrix /= transition_matrix.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Create the Markov Chain\n",
    "mc = MarkovChain(transition_matrix, ['0', '1'])\n",
    "\n",
    "# Function to predict the next state\n",
    "def predict_next_state(mc, current_state):\n",
    "    return np.random.choice(mc.states, p=mc.p[int(current_state)])\n",
    "\n",
    "# Predict the next state for the test data\n",
    "test_data = test[\"point_victor\"].values\n",
    "\n",
    "# Get the last state from the training data\n",
    "last_state_train = train_data[-1]\n",
    "\n",
    "# Generate the first prediction from the last state of the training data\n",
    "first_prediction = predict_next_state(mc, last_state_train)\n",
    "\n",
    "# Generate the rest of the predictions from the test data\n",
    "predictions_markov = [predict_next_state(mc, state) for state in test_data[:-1]]\n",
    "\n",
    "# Insert the first prediction at the beginning of the predictions list\n",
    "predictions_markov.insert(0, first_prediction)\n",
    "\n",
    "#print(predictions)\n",
    "#print(test_data[1:])\n",
    "\n",
    "# Calculate the RMSE\n",
    "rmse_markov = mean_squared_error(test_data[:], np.array(predictions_markov).astype(float), squared=False)\n",
    "\n",
    "print(rmse_markov)\n",
    "\n",
    "# Calculate the number of same values\n",
    "same_values = (test_data[:] == np.array(predictions_markov).astype(float)).sum()\n",
    "\n",
    "# Calculate the percentage\n",
    "percentage_markov = same_values / len(test_data[:]) * 100\n",
    "\n",
    "print(f\"Number of same values: {same_values}\")\n",
    "print(f\"Percentage: {percentage_markov}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.7637626158259734\n",
      "Number of same values: 25\n",
      "Percentage: 41.66666666666667%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ARIMA\n",
    "model_arima = ARIMA(train[\"point_victor\"], order=(5,1,0))\n",
    "model_arima_fit = model_arima.fit()\n",
    "predictions_arima = model_arima_fit.forecast(steps=len(test))\n",
    "\n",
    "\n",
    "#print(forecast_output)\n",
    "#print(test[\"point_victor\"])\n",
    "rmse_arima = sqrt(mean_squared_error(test[\"point_victor\"], predictions_arima))\n",
    "#print(rmse_arima)\n",
    "\n",
    "# Convert the ARIMA predictions\n",
    "forecast_output_binary = np.where(predictions_arima > 0.5, 1, 0)\n",
    "\n",
    "# Calculate the RMSE\n",
    "rmse_arima_binary = sqrt(mean_squared_error(test[\"point_victor\"], forecast_output_binary))\n",
    "\n",
    "# Calculate the number of same values\n",
    "same_values_arima = (test[\"point_victor\"] == forecast_output_binary).sum()\n",
    "\n",
    "# Calculate the percentage\n",
    "percentage_arima = same_values_arima / len(test[\"point_victor\"]) * 100\n",
    "\n",
    "print(f\"RMSE: {rmse_arima_binary}\")\n",
    "print(f\"Number of same values: {same_values_arima}\")\n",
    "print(f\"Percentage: {percentage_arima}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49821780127898274\n",
      "RMSE: 0.6454972243679028\n",
      "Number of same values: 35\n",
      "Percentage: 58.333333333333336%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# SARIMA\n",
    "model_sarima = SARIMAX(train[\"point_victor\"], order=(1, 1, 1), seasonal_order=(0, 0, 0, 0))\n",
    "model_sarima_fit = model_sarima.fit(disp=0)\n",
    "predictions_sarima = model_sarima_fit.forecast(steps=len(test))\n",
    "\n",
    "rmse_sarima = sqrt(mean_squared_error(test[\"point_victor\"], predictions_sarima))\n",
    "print(rmse_sarima)\n",
    "\n",
    "# Convert the SARIMA predictions\n",
    "predictions_sarima_binary = np.where(predictions_sarima > 0.5, 1, 0)\n",
    "\n",
    "# Calculate the RMSE\n",
    "rmse_sarima_binary = sqrt(mean_squared_error(test[\"point_victor\"], predictions_sarima_binary))\n",
    "\n",
    "# Calculate the number of same values\n",
    "same_values_sarima = (test[\"point_victor\"] == predictions_sarima_binary).sum()\n",
    "\n",
    "# Calculate the percentage\n",
    "percentage_sarima = same_values_sarima / len(test[\"point_victor\"]) * 100\n",
    "\n",
    "print(f\"RMSE: {rmse_sarima_binary}\")\n",
    "print(f\"Number of same values: {same_values_sarima}\")\n",
    "print(f\"Percentage: {percentage_sarima}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the weights\n",
    "total_weight=(abs(percentage_markov / 100 - 0.5) + abs(percentage_arima / 100 - 0.5) + abs(percentage_sarima / 100 - 0.5))\n",
    "weight_markov = abs(percentage_markov / 100 - 0.5) / total_weight\n",
    "weight_arima = abs(percentage_arima / 100 - 0.5) / total_weight\n",
    "weight_sarima = abs(percentage_sarima / 100 - 0.5)/ total_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Reverse the predictions if the accuracy is less than 50%\n",
    "if percentage_markov < 50:\n",
    "    predictions_markov = [1.0 - float(p) for p in predictions_markov]\n",
    "if percentage_arima < 50:\n",
    "    predictions_arima = [1.0 - p for p in predictions_arima]\n",
    "if percentage_sarima < 50:\n",
    "    predictions_sarima = [1.0 - p for p in predictions_sarima]\n",
    "\n",
    "print(predictions_markov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "60\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "print(len(predictions_markov))\n",
    "print(len(predictions_arima))\n",
    "print(len(predictions_sarima))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.6191391873668903\n",
      "Number of same values: 37\n",
      "Percentage: 61.66666666666667%\n",
      "Percentage of Markov: 48.333333333333336%\n",
      "Percentage of ARIMA: 41.66666666666667%\n",
      "Percentage of SARIMA: 58.333333333333336%\n"
     ]
    }
   ],
   "source": [
    "# Convert lists to numpy arrays\n",
    "predictions_markov = np.array(predictions_markov).astype(float)\n",
    "predictions_arima = np.array(predictions_arima)\n",
    "predictions_sarima = np.array(predictions_sarima)\n",
    "\n",
    "# Calculate the combined predictions\n",
    "combined_predictions = weight_markov * predictions_markov + weight_arima * predictions_arima + weight_sarima * predictions_sarima\n",
    "\n",
    "# Convert the combined predictions to binary\n",
    "combined_predictions_binary = np.where(combined_predictions > 0.5, 1, 0)\n",
    "\n",
    "# Calculate the RMSE\n",
    "rmse_combined = sqrt(mean_squared_error(test[\"point_victor\"], combined_predictions_binary))\n",
    "\n",
    "# Calculate the number of same values\n",
    "same_values_combined = (test[\"point_victor\"] == combined_predictions_binary).sum()\n",
    "\n",
    "# Calculate the percentage\n",
    "percentage_combined = same_values_combined / len(test[\"point_victor\"]) * 100\n",
    "\n",
    "print(f\"RMSE: {rmse_combined}\")\n",
    "print(f\"Number of same values: {same_values_combined}\")\n",
    "print(f\"Percentage: {percentage_combined}%\")\n",
    "\n",
    "#list all three alone percentage\n",
    "print(f\"Percentage of Markov: {percentage_markov}%\")\n",
    "print(f\"Percentage of ARIMA: {percentage_arima}%\")\n",
    "print(f\"Percentage of SARIMA: {percentage_sarima}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE values:\n",
      "Markov Chain:  0.7187952884282609\n",
      "ARIMA:  0.5349782365315047\n",
      "SARIMA:  0.49821780127898274\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print RMSE values\n",
    "print('RMSE values:')\n",
    "print('Markov Chain: ', rmse_markov)\n",
    "print('ARIMA: ', rmse_arima)\n",
    "print('SARIMA: ', rmse_sarima)\n",
    "#print('LSTM: ', rmse_lstm)\n",
    "\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
